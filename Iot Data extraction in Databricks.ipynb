{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect with Azure Gen2 data lake and mount it on Databricks\n",
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "       \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "       \"fs.azure.account.oauth2.client.id\": \"XXXXXXXXXXXXXXXXX\",\n",
    "       \"fs.azure.account.oauth2.client.secret\": \"XXXXXXXXXXXXX\",\n",
    "       \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/XXXXXXXXXXXXXX/oauth2/token\",\n",
    "       \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"}\n",
    "dbutils.fs.mount(\n",
    "source = \"abfss://XXXX@dydstorage2021gen2.dfs.core.windows.net/\",\n",
    "mount_point = \"/mnt/newfolder\",\n",
    "extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the mounted folders \n",
    "display(dbutils.fs.mounts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List all files within a specific folder\n",
    "filepath=('/mnt/xxxx/')\n",
    "dbutils.fs.ls(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all the files within the folde with specific name format\n",
    "df = (spark.read.csv(filepath+\"xxxxx\"+\"*\", header=\"true\", inferSchema=\"true\"))\n",
    "# - First line of file is a header\n",
    "# - Automatically infer the schema of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape of the dataframe\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view and download data if required\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count any variable, for example date \n",
    "df.count(\"Date\").show()\n",
    "#frequency count for any variable \n",
    "pd.Series(df[\"totalsensors\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform data wrangling####\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark\n",
    "df.select(to_timestamp(df.date_time, 'yyyy-MM-dd HH:mm:ss').alias('DateTime')).collect()\n",
    "df=df.withColumn(\"DateTime\",to_timestamp(\"date_time\"))\n",
    "df=df.withColumn(\"Date\",to_date(\"DateTime\"))\n",
    "df=df.toPandas()\n",
    "df=df.select(col(\"Date\"), to_date(col(\"DateTime\")).alias(\"to_date\"))\n",
    "df_before=df.filter(df.Date > \"2020-03-18\")\n",
    "df_during=df.filter(df.Date <= \"2020-03-18\")\n",
    "df_before.show()\n",
    "df_during.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write result datafarme to storage on Azure\n",
    "df_before.write \\\n",
    ".format(\"jdbc\")\\\n",
    ".option(\"url\", \"jdbc:sqlserver://xxxx-data.database.windows.net:1433;database=xxxx\")\\\n",
    ".option(\"dbtable\", \"dbo.ab_df\")\\\n",
    ".option(\"user\", \"xxxx@uxxx.ca\")\\\n",
    ".option(\"password\", \"xxxxx\")\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save result as table on Azure\n",
    "df_before.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"Identifier\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"/mnt/xxxx/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save result as table on databricks\n",
    "df.write.saveAsTable(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from databricks table\n",
    "df = spark.sql(\"select * from df\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
